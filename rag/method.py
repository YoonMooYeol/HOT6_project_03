from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_community.document_loaders import CSVLoader
import os
from glob import glob
import uuid
import pickle
from .models import RAG_DB
import asyncio
from typing import List
from langchain.prompts import ChatPromptTemplate
from django.conf import settings
from tqdm import tqdm

from dotenv import load_dotenv

load_dotenv()

class RAGProcessor:
    TEMP_DIR = "data/temp_embeddings"
    DB_DIR = os.path.join(settings.BASE_DIR, "embeddings", "chroma_db")

    @staticmethod
    def load_and_preprocess_csv(csv_pattern):
        """CSV íŒŒì¼ë“¤ì„ ì°¾ì•„ì„œ ë°˜í™˜."""
        csv_files = glob(csv_pattern)
        if not csv_files:
            print(f"CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_pattern}")
            return None
        print(f"ì²˜ë¦¬í•  CSV íŒŒì¼: {len(csv_files)}ê°œ")
        for file in csv_files:
            print(f"- {file}")
        return csv_files

    @staticmethod
    def filter_processed_files(csv_files):
        """ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì„ ì œì™¸í•˜ê³  ìƒˆë¡œìš´ íŒŒì¼ ëª©ë¡ë§Œ ë°˜í™˜."""
        processed_files = set(RAG_DB.objects.values_list('file_path', flat=True))
        new_files = [f for f in csv_files if f not in processed_files]
        print(f"ì²˜ë¦¬í•  ìƒˆë¡œìš´ CSV íŒŒì¼: {len(new_files)}ê°œ")
        return new_files

    @staticmethod
    def initialize_chroma_db():
        """Chroma DBë¥¼ ì´ˆê¸°í™”í•˜ê±°ë‚˜ ê¸°ì¡´ DBë¥¼ ë¡œë“œ."""
        db_dir = RAGProcessor.DB_DIR
        print("\n=== Chroma DB ìƒíƒœ ===")
        print(f"ì‚¬ìš© ì¤‘ì¸ DB ê²½ë¡œ: {db_dir}")
        print(f"ì ˆëŒ€ ê²½ë¡œ: {os.path.abspath(db_dir)}")
        
        # DB íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        if os.path.exists(f"{db_dir}/chroma.sqlite3"):
            print(f"chroma.sqlite3 íŒŒì¼ í¬ê¸°: {os.path.getsize(f'{db_dir}/chroma.sqlite3')} bytes")
        
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small", chunk_size=1000)

        if os.path.exists(db_dir) and os.path.exists(f"{db_dir}/chroma.sqlite3"):
            print("ê¸°ì¡´ Chroma DB ë¡œë“œ ì¤‘...")
            vectorstore = Chroma(
                persist_directory=db_dir,
                embedding_function=embeddings,
                collection_name="korean_dialogue"
            )
            existing_ids = set(vectorstore._collection.get()['ids'])
            print(f"ê¸°ì¡´ ë¬¸ì„œ ìˆ˜: {len(existing_ids)}")
        else:
            print("ìƒˆë¡œìš´ Chroma DB ìƒì„±")
            vectorstore = None
            existing_ids = set()

        return vectorstore, existing_ids

    @staticmethod
    def load_csv_with_metadata(csv_file):
        """CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ë©”íƒ€ë°ì´í„° ì—´ì„ ì¶”ê°€."""
        print(f"\níŒŒì¼ ì²˜ë¦¬ ì¤‘: {csv_file}")
        loader = CSVLoader(file_path=csv_file, metadata_columns=['emotion'])
        return loader.load()

    @staticmethod
    def filter_new_documents(docs, existing_ids, csv_file):
        """ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ë¬¸ì„œë¥¼ ì œì™¸í•˜ê³  ìƒˆ ë¬¸ì„œë§Œ í•„í„°ë§."""
        new_docs = []
        for idx, doc in enumerate(docs):
            doc.metadata['source_file'] = os.path.basename(csv_file)
            unique_id = str(uuid.uuid4())
            doc_id = f"doc_{unique_id}_{idx}"
            if doc_id not in existing_ids:
                doc.metadata['doc_id'] = doc_id
                new_docs.append(doc)
        print(f"ìƒˆë¡œìš´ ë¬¸ì„œ ë°œê²¬: {len(new_docs)}ê°œ")
        return new_docs

    @staticmethod
    def split_documents(docs):
        """ë¬¸ì„œë¥¼ ì²­í¬ë¡œ ë¶„í• ."""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            length_function=len,
            separators=["\n\n", "\n", " ", ""]
        )
        splits = text_splitter.split_documents(docs)
        print(f"ë¶„í•  ì™„ë£Œ: {len(splits)}ê°œ ì²­í¬")
        return splits

    @staticmethod
    def prepare_data_for_chroma(splits):
        """Chroma DBì— ì €ì¥í•  í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„°, ID ì¤€ë¹„."""
        texts, metadatas, ids = [], [], []
        for doc in splits:
            texts.append(f"content: {doc.page_content}")
            metadatas.append({
                "emotion": doc.metadata.get('emotion', ''),
            })
            ids.append(doc.metadata.get('doc_id'))

        return texts, metadatas, ids

    @staticmethod
    async def create_embeddings_async(texts: List[str], pbar: tqdm, batch_size: int = 20, concurrent_tasks: int = 5) -> List[List[float]]:
        """í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ì˜ ì„ë² ë”©ì„ ë¹„ë™ê¸°ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
        embedding_function = OpenAIEmbeddings(
            model="text-embedding-3-small",
            chunk_size=1000
        )
        
        all_embeddings = []
        batches = [texts[i:i + batch_size] for i in range(0, len(texts), batch_size)]
        semaphore = asyncio.Semaphore(concurrent_tasks)
        async def process_batch(batch):
            async with semaphore:
                await asyncio.sleep(0.1)
                return await embedding_function.aembed_documents(batch)
        tasks = [process_batch(batch) for batch in batches]
        for coro in asyncio.as_completed(tasks):
            embeddings = await coro
            if embeddings:
                all_embeddings.extend(embeddings)
                pbar.update(batch_size)
        return all_embeddings

    @staticmethod
    def get_optimal_embedding_params(num_texts: int) -> (int, int):
        """
        ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ì´ ê°œìˆ˜(num_texts)ì™€ CPU ì½”ì–´ ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìµœì ì˜ batch_sizeì™€
        concurrent_tasks ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

        Returns:
            tuple: (batch_size, concurrent_tasks)
        """
        import multiprocessing
        cpu_count = multiprocessing.cpu_count()
        # í…ìŠ¤íŠ¸ ìˆ˜ì— ë”°ë¼ ë°°ì¹˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” ê°„ë‹¨í•œ heuristic
        if num_texts < 100:
            batch_size = 10
        elif num_texts < 1000:
            batch_size = 20
        else:
            batch_size = 50

        # ë™ì‹œ ì‹¤í–‰ íƒœìŠ¤í¬ëŠ” CPU ì½”ì–´ ìˆ˜ì™€ ìƒí•œê°’ 10ì„ ê³ ë ¤
        concurrent_tasks = min(10, cpu_count)
        print(
            f"Optimal Parameters determined: batch_size={batch_size}, "
            f"concurrent_tasks={concurrent_tasks} based on {num_texts} texts and {cpu_count} CPUs"
        )
        return batch_size, concurrent_tasks

    @staticmethod
    def create_embeddings(texts: List[str]) -> List[List[float]]:
        """ë™ê¸° ë°©ì‹ìœ¼ë¡œ ë¹„ë™ê¸° ì„ë² ë”© ìƒì„±ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        ì…ë ¥ í…ìŠ¤íŠ¸ì˜ ìˆ˜ì— ë”°ë¼ ìµœì ì˜ batch_sizeì™€ concurrent_tasksë¥¼ ê³„ì‚°í•˜ì—¬ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
        """
        batch_size, concurrent_tasks = RAGProcessor.get_optimal_embedding_params(len(texts))
        with tqdm(total=len(texts), desc="ì„ë² ë”© ìƒì„± ì¤‘") as pbar:
            embeddings = asyncio.run(
                RAGProcessor.create_embeddings_async(texts, pbar, batch_size, concurrent_tasks)
            )
        return embeddings

    @staticmethod
    def process_files(csv_files: List[str], existing_ids: set, vectorstore, db_dir: str):
        """CSV íŒŒì¼ë“¤ì„ ì²˜ë¦¬í•˜ê³  ì§„í–‰ìƒí™©ì„ ì‹œê°í™”í•©ë‹ˆë‹¤."""
        total_new_docs = 0
        processed_count = 0

        print("\n=== CSV íŒŒì¼ ì²˜ë¦¬ ì‹œì‘ ===")
        for csv_file in tqdm(csv_files, desc="ğŸ“‚ CSV íŒŒì¼ ì²˜ë¦¬"):
            try:
                # CSV íŒŒì¼ ë¡œë“œ ë° ë©”íƒ€ë°ì´í„° ì¶”ê°€
                docs = RAGProcessor.load_csv_with_metadata(csv_file)
                if not docs:
                    continue

                # ìƒˆ ë¬¸ì„œ í•„í„°ë§
                new_docs = RAGProcessor.filter_new_documents(docs, existing_ids, csv_file)
                if not new_docs:
                    continue

                # ë¬¸ì„œ ë¶„í• 
                total_new_docs += len(new_docs)
                splits = RAGProcessor.split_documents(new_docs)
                
                # ë°ì´í„° ì¤€ë¹„
                texts, metadatas, ids = RAGProcessor.prepare_data_for_chroma(splits)
                
                print(f"\nğŸ“„ [{os.path.basename(csv_file)}] ì²˜ë¦¬ ì¤‘...")
                print(f"   - í…ìŠ¤íŠ¸ ìˆ˜: {len(texts)}ê°œ")
                
                # ì„ì‹œ ì €ì¥ëœ ì„ë² ë”© í™•ì¸
                temp_embeddings = RAGProcessor.load_temp_embeddings(csv_file)
                
                if temp_embeddings is not None:
                    print("ğŸ’¾ ê¸°ì¡´ ì„ì‹œ ì„ë² ë”© ì‚¬ìš©")
                    embeddings = temp_embeddings
                else:
                    print("ğŸ”„ ìƒˆë¡œìš´ ì„ë² ë”© ìƒì„± ì‹œì‘")
                    embeddings = RAGProcessor.create_embeddings(texts)
                    RAGProcessor.save_temp_embeddings(csv_file, embeddings)
                
                # Chroma DB ì—…ë°ì´íŠ¸
                vectorstore = RAGProcessor.update_chroma_db(
                    vectorstore, texts, embeddings, metadatas, ids, db_dir
                )
                
                # ì²˜ë¦¬ ì™„ë£Œ ê¸°ë¡
                RAGProcessor.save_processed_file_info(csv_file)
                processed_count += 1
                print(f"âœ… [{os.path.basename(csv_file)}] ì²˜ë¦¬ ì™„ë£Œ\n")

            except Exception as e:
                print(f"âŒ íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({os.path.basename(csv_file)}): {e}")
                continue

        return vectorstore, total_new_docs, processed_count

    @staticmethod
    def update_chroma_db(vectorstore, texts, embeddings, metadatas, ids, db_dir):
        """Chroma DBì— ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì¶”ê°€í•˜ê³  ì§„í–‰ìƒí™©ì„ í‘œì‹œí•©ë‹ˆë‹¤."""
        MAX_BATCH_SIZE = 5000

        if vectorstore is None:
            print("ğŸ”¨ ìƒˆë¡œìš´ Chroma DB ìƒì„± ì¤‘...")
            embedding_function = OpenAIEmbeddings(model="text-embedding-3-small")
            vectorstore = Chroma(
                persist_directory=db_dir,
                embedding_function=embedding_function,
                collection_name="korean_dialogue"
            )

        total_batches = (len(texts) + MAX_BATCH_SIZE - 1) // MAX_BATCH_SIZE
        print(f"ğŸ“¦ Chroma DB ì—…ë°ì´íŠ¸ ì‹œì‘ (ì´ {total_batches}ê°œ ë°°ì¹˜)")
        
        for i in tqdm(range(0, len(texts), MAX_BATCH_SIZE), desc="ğŸ’« DB ì—…ë°ì´íŠ¸"):
            end_idx = min(i + MAX_BATCH_SIZE, len(texts))
            vectorstore._collection.add(
                embeddings=embeddings[i:end_idx],
                documents=texts[i:end_idx],
                metadatas=metadatas[i:end_idx],
                ids=ids[i:end_idx]
            )
        
        print(f"âœ¨ DB ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ {len(texts)}ê°œ ë¬¸ì„œ)")
        return vectorstore

    @staticmethod
    async def async_update_chroma_db(vectorstore, texts, embeddings, metadatas, ids, db_dir):
        """
        ë¹„ë™ê¸° ë°©ì‹ìœ¼ë¡œ Chroma DBë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        ìµœëŒ€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ 5000ê°œë¥¼ ê³ ë ¤í•˜ì—¬ vectorstore._collection.add í˜¸ì¶œì„
        asyncio.to_threadë¡œ ê°ì‹¸ì„œ ë™ì‹œì— ì‹¤í–‰í•©ë‹ˆë‹¤.

        Returns:
            ì—…ë°ì´íŠ¸ëœ vectorstore ì¸ìŠ¤í„´ìŠ¤.
        """
        MAX_BATCH_SIZE = 5000
        total_batches = (len(texts) + MAX_BATCH_SIZE - 1) // MAX_BATCH_SIZE
        print(f"ğŸ“¦ ë¹„ë™ê¸° Chroma DB ì—…ë°ì´íŠ¸ ì‹œì‘ (ì´ {total_batches}ê°œ ë°°ì¹˜)")
        tasks = []
        for i in range(0, len(texts), MAX_BATCH_SIZE):
            end_idx = min(i + MAX_BATCH_SIZE, len(texts))
            tasks.append(
                asyncio.to_thread(
                    vectorstore._collection.add,
                    embeddings=embeddings[i:end_idx],
                    documents=texts[i:end_idx],
                    metadatas=metadatas[i:end_idx],
                    ids=ids[i:end_idx]
                )
            )
        await asyncio.gather(*tasks)
        print(f"âœ¨ ë¹„ë™ê¸° DB ì—…ë°ì´íŠ¸ ì™„ë£Œ (ì´ {len(texts)}ê°œ ë¬¸ì„œ)")
        return vectorstore

    @staticmethod
    def save_processed_file_info(csv_file):
        """ì²˜ë¦¬ëœ íŒŒì¼ ì •ë³´ë¥¼ DBì— ì €ì¥."""
        RAG_DB.objects.create(file_name=os.path.basename(csv_file), file_path=csv_file)

    @staticmethod
    def get_temp_embedding_path(file_name):
        """ì„ì‹œ ì„ë² ë”© íŒŒì¼ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        if not os.path.exists(RAGProcessor.TEMP_DIR):
            os.makedirs(RAGProcessor.TEMP_DIR)
        return os.path.join(RAGProcessor.TEMP_DIR, f"{os.path.basename(file_name)}.pkl")

    @staticmethod
    def save_temp_embeddings(file_name, data):
        """ì„ë² ë”© ë°ì´í„°ë¥¼ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        temp_path = RAGProcessor.get_temp_embedding_path(file_name)
        with open(temp_path, 'wb') as f:
            pickle.dump(data, f)

    @staticmethod
    def load_temp_embeddings(file_name):
        """ì„ì‹œ ì €ì¥ëœ ì„ë² ë”© ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
        temp_path = RAGProcessor.get_temp_embedding_path(file_name)
        if os.path.exists(temp_path):
            with open(temp_path, 'rb') as f:
                return pickle.load(f)
        return None

    @staticmethod
    def process_conversation_json(conversation, existing_ids, vectorstore):
        """
        conversation: ëŒ€í™” JSONìœ¼ë¡œ, "info"ì™€ "utterances" í‚¤ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
        existing_ids: ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œì˜ ID ì§‘í•©
        vectorstore: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
        
        ì´ í•¨ìˆ˜ëŠ” conversationì—ì„œ ê° utteranceì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , 
        ê¸°ì¡´ì— ì²˜ë¦¬ëœ ë¬¸ì„œëŠ” ê±´ë„ˆë›´ í›„ ìƒˆë¡œìš´ í…ìŠ¤íŠ¸ë“¤ì„ vectorstoreì— ì¶”ê°€í•©ë‹ˆë‹¤.
        
        Returns:
            vectorstore, new_docs (int), processed_count (int)
        """
        new_docs = 0
        processed_count = 0
        texts = []
        metadatas = []
        # ì˜ˆì‹œ: ê° utteranceëŠ” "text" í•„ë“œë¥¼ í¬í•¨í•œ dictë¼ê³  ê°€ì •
        for utter in conversation.get("utterances", []):
            text = utter.get("text", "").strip()
            if not text:
                continue
            doc_id = hash(text)  # ë‹¨ìˆœ í•´ì‹œ ì‚¬ìš©; ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ë” ì•ˆì „í•œ ë°©ë²• ì‚¬ìš© ê¶Œì¥
            if doc_id in existing_ids:
                continue
            texts.append(text)
            metadata = {
                "source": conversation.get("info", {}).get("source", "json"),
                "doc_id": doc_id
            }
            metadatas.append(metadata)
            new_docs += 1
            processed_count += 1
        
        if texts:
            # vectorstoreì— í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ë©”ì„œë“œ ëª…ì¹­ì€ ì‹¤ì œ êµ¬í˜„ì— ë§ê²Œ ìˆ˜ì •)
            vectorstore.add_texts(texts, metadatas)
        
        return vectorstore, new_docs, processed_count

class RAGQuery:
    @staticmethod
    def create_qa_chain():
        """ê³µìœ ëœ DB_DIRì„ ì‚¬ìš©í•˜ì—¬ QA ì²´ì¸ ìƒì„±"""
        db_dir = RAGProcessor.DB_DIR
        vectorstore = Chroma(
            persist_directory=db_dir,
            embedding_function=OpenAIEmbeddings(
                model="text-embedding-3-small"
            ),
            collection_name="korean_dialogue"
        )
        
        # í•„í„° ì œê±°í•˜ê³  í…ŒìŠ¤íŠ¸
        retriever = vectorstore.as_retriever(
            search_kwargs={"k": 10},  # ìƒìœ„ 10ê°œ ê²°ê³¼
            filter={"emotion": "happy"},
        )
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì»¬ë ‰ì…˜ ì •ë³´ ì¶œë ¥
        print(f"ì»¬ë ‰ì…˜ ë‚´ ë¬¸ì„œ ìˆ˜: {vectorstore._collection.count()}")
        
        llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=1.1
        )

        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìˆ˜ì •: ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ ë¦¬íŠ¸ë¦¬ë¸Œëœ ë¬¸ì„œë¥¼ ë³„ë„ì˜ í‚¤ë¡œ ì „ë‹¬
        template = """Given the chat history and the retrieved context, rephrase the partner's harsh message into a gentle, warm, and loving tone that fits naturally into your ongoing conversation.
        
        Chat History:
        {chat_history}
        
        Retrieved Context:
        {retrieved_context}
        
        Partner's harsh message:
        {question}
        
        Instructions:
          1. Carefully analyze the chat history to grasp the emotional cues.
          2. Incorporate relevant context from the retrieved documents.
          3. Rephrase the partner's message, keeping its original meaning while softening the tone into a caring and respectful manner.
          4. Provide three alternative rephrasings separated by pipes (|).
          5. Each alternative should be concise (aim for around 15-30 words) and crafted for a loving conversation.
          6. Always respond in Korean.
        
        Response Format:
        Alternative1 | Alternative2 | Alternative3
        """
        
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm
        
        return retriever, chain

    @staticmethod
    def get_answer(question: str):
        # chat_message í…Œì´ë¸”(ëª¨ë¸)ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸° (ChatMessage ëª¨ë¸ì´ ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤)
        from chat.models import ChatMessage

        # ì‹œê°„ìˆœìœ¼ë¡œ ì •ë ¬ëœ ì „ì²´ ì±„íŒ… ë©”ì‹œì§€ë¡œ ëŒ€í™” ë§¥ë½ êµ¬ì„±
        messages = ChatMessage.objects.all().order_by('timestamp')
        chat_history = "\n".join([f"{msg.sender}: {msg.content}" for msg in messages])

        # ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì¶”ê°€ì ì¸ ë¬¸ì„œ(ëŒ€í™” ê´€ë ¨ ë¬¸ë§¥) ê°€ì ¸ì˜¤ê¸°
        retriever, chain = RAGQuery.create_qa_chain()
        retrieved_docs = retriever.invoke(question)
        retrieved_context = "\n".join([doc.page_content for doc in retrieved_docs])

        print(f"Chat History: {chat_history}")
        print(f"Retrieved Context: {retrieved_context}")
        result = chain.invoke({
            "chat_history": chat_history,
            "retrieved_context": retrieved_context,
            "question": question
        })
        return result.content

